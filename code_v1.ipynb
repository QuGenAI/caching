{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b547b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import redis\n",
    "from redis.commands.search.field import TextField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from redisvl.utils.vectorize import HFTextVectorizer, BaseVectorizer\n",
    "from redisvl.extensions.cache.embeddings import EmbeddingsCache\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD=''\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "LLM_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "VECTOR_DIM = 768\n",
    "INDEX_NAME = \"rag_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f2ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DOCUMENTS = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\",\n",
    "    \"Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\",\n",
    "    \"Vector databases store and retrieve data based on vector similarity. They are essential for semantic search, recommendation systems, and machine learning applications.\",\n",
    "    \"Redis is an in-memory data structure store that provides high-performance caching and real-time data processing capabilities. RedisVL extends Redis with semantic search capabilities.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingEngine:\n",
    "    def __init__(self, model_name: str = EMBEDDING_MODEL):\n",
    "        \"\"\"Initialize embedding engine with specified model\"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEngine:\n",
    "    def __init__(self, model_name: str = LLM_MODEL):\n",
    "        \"\"\"Initialize LLM with 4-bit quantization\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 100, cache_key: str = None) -> str:\n",
    "        \"\"\"Generate text with optional caching\"\"\"\n",
    "        # Simple LM-Cache simulation (for production, integrate actual LMCache)\n",
    "        if cache_key:\n",
    "            cached_result = self._get_cache(cache_key)\n",
    "            if cached_result:\n",
    "                return cached_result\n",
    "        \n",
    "        # Generate text\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        # Ensure max_length accounts for input length\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        # Decode output\n",
    "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Store in cache\n",
    "        if cache_key:\n",
    "            self._set_cache(cache_key, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_cache(self, key: str) -> str:\n",
    "        \"\"\"Placeholder for LM-Cache retrieval\"\"\"\n",
    "        return None\n",
    "    \n",
    "    def _set_cache(self, key: str, value: str):\n",
    "        \"\"\"Placeholder for LM-Cache storage\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisSimpleVectorStore:\n",
    "    def __init__(self, host: str = REDIS_HOST, port: int = REDIS_PORT):\n",
    "        \"\"\"Initialize Redis vector store and create index if not exists\"\"\"\n",
    "        self.client = redis.Redis(host=host, port=port, decode_responses=False)\n",
    "        self.embedding_engine = EmbeddingEngine()\n",
    "        self._create_index()\n",
    "    \n",
    "    def _create_index(self):\n",
    "        \"\"\"Create Redis index for storing documents\"\"\"\n",
    "        try:\n",
    "            # self.client.ft(INDEX_NAME).info()\n",
    "            self.client.flushdb()\n",
    "            print(f\"‚úì Index '{INDEX_NAME}' already exists, reusing...\")\n",
    "            return\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        schema = (\n",
    "            TextField(\"content\"),\n",
    "        )\n",
    "        \n",
    "        definition = IndexDefinition(index_type=IndexType.HASH)\n",
    "        self.client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)\n",
    "        print(f\"‚úì Created index '{INDEX_NAME}'\")\n",
    "    \n",
    "    def add_documents(self, docs: List[str]):\n",
    "        \"\"\"Add documents with embeddings to vector store\"\"\"\n",
    "        embeddings = self.embedding_engine.embed(docs)\n",
    "        \n",
    "        for idx, (doc, embedding) in enumerate(zip(docs, embeddings)):\n",
    "            doc_id = f\"doc:{idx}\".encode()\n",
    "            embedding_bytes = np.array(embedding, dtype=np.float32).tobytes()\n",
    "            \n",
    "            self.client.hset(\n",
    "                doc_id,\n",
    "                mapping={\n",
    "                    b\"content\": doc.encode(),\n",
    "                    b\"embedding\": embedding_bytes\n",
    "                }\n",
    "            )\n",
    "        print(f\"‚úì Added {len(docs)} documents to vector store\")\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Semantic search with caching\"\"\"\n",
    "        \n",
    "        \n",
    "        # Check semantic cache\n",
    "        cache_key = f\"semantic_cache:{query}\".encode()\n",
    "        cached_results = self.client.get(cache_key)\n",
    "        \n",
    "        if cached_results:\n",
    "            print(f\"‚úì Semantic cache hit for: '{query}'\")\n",
    "            return json.loads(cached_results.decode())\n",
    "        \n",
    "        query_embedding = self.embedding_engine.embed([query])[0]\n",
    "        # Manual vector similarity search\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        doc_scores = []\n",
    "        \n",
    "        # Get all documents\n",
    "        keys = self.client.keys(b\"doc:*\")\n",
    "        \n",
    "        for key in keys:\n",
    "            doc_data = self.client.hgetall(key)\n",
    "            if b\"embedding\" in doc_data:\n",
    "                # Deserialize embedding\n",
    "                stored_embedding = np.frombuffer(\n",
    "                    doc_data[b\"embedding\"], dtype=np.float32\n",
    "                )\n",
    "                # Calculate similarity\n",
    "                similarity = cosine_similarity(\n",
    "                    [query_embedding],\n",
    "                    [stored_embedding]\n",
    "                )[0][0]\n",
    "                \n",
    "                doc_scores.append({\n",
    "                    \"id\": key.decode(),\n",
    "                    \"content\": doc_data.get(b\"content\", b\"\").decode(),\n",
    "                    \"score\": float(similarity)\n",
    "                })\n",
    "        \n",
    "        # Sort by score and get top_k\n",
    "        doc_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        formatted_results = doc_scores[:top_k]\n",
    "        \n",
    "        # Cache semantic results\n",
    "        self.client.setex(\n",
    "            cache_key,\n",
    "            3600,  # 1 hour TTL\n",
    "            json.dumps(formatted_results).encode()\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Semantic search completed and cached for: '{query}'\")\n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize RAG pipeline with vector store, LLM, and embedding engine\"\"\"\n",
    "        self.vector_store = RedisSimpleVectorStore()\n",
    "        self.llm = LLMEngine()\n",
    "        self.embedding_engine = EmbeddingEngine()\n",
    "    \n",
    "    def initialize(self, documents: List[str]):\n",
    "        \"\"\"Initialize RAG with documents\"\"\"\n",
    "        self.vector_store.add_documents(documents)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Execute RAG query with retrieval and generation\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.vector_store.semantic_search(question, top_k=top_k)\n",
    "        \n",
    "        # Build context from retrieved documents\n",
    "        context = \"\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "        \n",
    "        # Create prompt with context\n",
    "        prompt = f\"\"\"Context:\\n{context}\\nQuestion:\\n{question}\\nAnswer:\\n\"\"\"\n",
    "        \n",
    "        # Generate answer with LLM caching\n",
    "        cache_key = f\"llm_cache:{question}\"\n",
    "        answer = self.llm.generate(prompt, max_tokens=150, cache_key=cache_key)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57725ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Index 'rag_index' already exists, reusing...\n",
      "‚úì Added 5 documents to vector store\n"
     ]
    }
   ],
   "source": [
    "rag = RAGPipeline()\n",
    "rag.initialize(RAG_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is machine learning?\"\n",
    "print(f\"\\n‚ùì Query: {query}\")\n",
    "result = rag.query(query)\n",
    "print(f\"\\nüìÑ Retrieved Documents ({len(result['retrieved_docs'])}): \")\n",
    "for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "    print(f\"  {i}. {doc['content'][:80]}...\")\n",
    "print(f\"\\nüí¨ Answer:\\n{result['answer']}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7eb3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "‚ùì Query: What is machine learning?\n",
      "‚úì Semantic search completed and cached for: 'What is machine learning?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ramsi\\anaconda3\\envs\\ai_gpu\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Retrieved Documents (3): \n",
      "  1. Machine learning is a subset of artificial intelligence that enables systems to ...\n",
      "  2. Deep learning uses neural networks with multiple layers to process data. It has ...\n",
      "  3. Natural language processing (NLP) is a branch of AI that helps computers underst...\n",
      "\n",
      "üí¨ Answer:\n",
      "Context:\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
      "Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\n",
      "Question:\n",
      "What is machine learning?\n",
      "Answer:\n",
      "A machine learning is a type of learning process in which the system improves over time by using data and experience. It is used for tasks such as pattern recognition, classification, and decision-making. The key is to learn from\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: How does deep learning work?\n",
      "‚úì Semantic search completed and cached for: 'How does deep learning work?'\n",
      "\n",
      "üìÑ Retrieved Documents (3): \n",
      "  1. Deep learning uses neural networks with multiple layers to process data. It has ...\n",
      "  2. Machine learning is a subset of artificial intelligence that enables systems to ...\n",
      "  3. Natural language processing (NLP) is a branch of AI that helps computers underst...\n",
      "\n",
      "üí¨ Answer:\n",
      "Context:\n",
      "Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\n",
      "Question:\n",
      "How does deep learning work?\n",
      "Answer:\n",
      "In the past, humans used to process data using machines. Now, deep learning uses neural networks with multiple layers to process data. The result is that the data is processed more efficiently.\n",
      "Answer:\n",
      "The answer is that\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ùì Query: What is machine learning?\n",
      "‚úì Semantic cache hit for: 'What is machine learning?'\n",
      "\n",
      "üìÑ Retrieved Documents (3): \n",
      "  1. Machine learning is a subset of artificial intelligence that enables systems to ...\n",
      "  2. Deep learning uses neural networks with multiple layers to process data. It has ...\n",
      "  3. Natural language processing (NLP) is a branch of AI that helps computers underst...\n",
      "\n",
      "üí¨ Answer:\n",
      "Context:\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
      "Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\n",
      "Question:\n",
      "What is machine learning?\n",
      "Answer:\n",
      "The answer is that machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "\n",
      "Question:\n",
      "What is the importance of machine learning?\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does deep learning work?\",\n",
    "    \"What is machine learning?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "for query in queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    result = rag.query(query)\n",
    "    print(f\"\\nüìÑ Retrieved Documents ({len(result['retrieved_docs'])}): \")\n",
    "    for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "        print(f\"  {i}. {doc['content'][:80]}...\")\n",
    "    print(f\"\\nüí¨ Answer:\\n{result['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97a70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Query: describe Machine learning\n",
      "‚úì Semantic search completed and cached for: 'describe Machine learning'\n",
      "\n",
      "üìÑ Retrieved Documents (3): \n",
      "  1. Machine learning is a subset of artificial intelligence that enables systems to ...\n",
      "  2. Deep learning uses neural networks with multiple layers to process data. It has ...\n",
      "  3. Natural language processing (NLP) is a branch of AI that helps computers underst...\n",
      "\n",
      "üí¨ Answer:\n",
      "Context:\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
      "Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\n",
      "Question:\n",
      "describe Machine learning\n",
      "Answer:\n",
      "The machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "The machine learning is a subset of artificial intelligence that enables systems to learn\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query=\"describe Machine learning\"\n",
    "print(f\"\\n‚ùì Query: {query}\")\n",
    "result = rag.query(query)\n",
    "print(f\"\\nüìÑ Retrieved Documents ({len(result['retrieved_docs'])}): \")\n",
    "for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "    print(f\"  {i}. {doc['content'][:80]}...\")\n",
    "print(f\"\\nüí¨ Answer:\\n{result['answer']}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9cfe0",
   "metadata": {},
   "source": [
    "# Semantic Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fec86c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "from redisvl.query import VectorQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisSemanticVectorStore:\n",
    "    def __init__(self, host: str = REDIS_HOST, port: int = REDIS_PORT):\n",
    "        \"\"\"Initialize Redis vector store and create index\"\"\"\n",
    "        self.client = redis.Redis(host=host, port=port, decode_responses=False)\n",
    "\n",
    "        self._create_index()\n",
    "\n",
    "    def _create_index(self):\n",
    "        \"\"\"Create Redis index for storing documents\"\"\"\n",
    "\n",
    "        index_name = \"redisvl\"\n",
    "\n",
    "        schema = {\n",
    "                \"index\": {\n",
    "                    \"name\": index_name,\n",
    "                    \"prefix\": \"doc\"\n",
    "                },\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"name\": \"doc_id\",\n",
    "                        \"type\": \"tag\",\n",
    "                        \"attrs\": {\n",
    "                            \"sortable\": True\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"content\",\n",
    "                        \"type\": \"text\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"embedding\",\n",
    "                        \"type\": \"vector\",\n",
    "                        \"attrs\": {\n",
    "                            \"dims\": 768,\n",
    "                            \"distance_metric\": \"cosine\",\n",
    "                            \"algorithm\": \"hnsw\",\n",
    "                            \"datatype\": \"float32\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "\n",
    "        self.client = SearchIndex.from_dict(schema, redis_url=REDIS_URL)\n",
    "        self.client.create(overwrite=True, drop=True)\n",
    "        print(f\"‚úì Created index '{index_name}'\")\n",
    "\n",
    "    def add_documents(self, docs: List[str], embedding_engine):\n",
    "        \"\"\"Add documents with embeddings to vector store\"\"\"\n",
    "        from redisvl.redis.utils import array_to_buffer\n",
    "        embedding = embedding_engine.embed_many(docs)\n",
    "        data = [\n",
    "            {\n",
    "                \"doc_id\": i,\n",
    "                \"content\": chunk,\n",
    "                \"embedding\": array_to_buffer(embedding[i], dtype=\"float32\"),\n",
    "            }\n",
    "            for i, chunk in enumerate(docs)\n",
    "        ]\n",
    "        keys = self.client.load(data, id_field=\"doc_id\")\n",
    "        print(f\"‚úì Added {len(keys)} documents to vector store\")\n",
    "\n",
    "    def semantic_search(self, query_embedding: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Semantic search with caching - using manual similarity\"\"\"\n",
    "\n",
    "        \n",
    "        # Check semantic cache\n",
    "        vector_query = VectorQuery(\n",
    "            vector=query_embedding,\n",
    "            vector_field_name=\"embedding\",\n",
    "            num_results=top_k,\n",
    "            return_fields=[\"doc_id\", \"content\"],\n",
    "            return_score=True,\n",
    "        )\n",
    "\n",
    "        print(str(vector_query))\n",
    "        result=self.client.query(vector_query)\n",
    "        return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemaRAGPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize RAG pipeline with vector store, LLM, and embedding engine\"\"\"\n",
    "        self.vector_store = RedisSemanticVectorStore()\n",
    "        self.llm = LLMEngine()\n",
    "        self.embedding_engine = HFTextVectorizer(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            cache=EmbeddingsCache(name=\"embedcache\", ttl=600, redis_url=REDIS_URL),\n",
    "        )\n",
    "\n",
    "        #  Initialize semantic cache\n",
    "        self.llmcache = SemanticCache(\n",
    "            name=\"cache\",\n",
    "            vectorizer=self.embedding_engine,\n",
    "            redis_url=REDIS_URL,\n",
    "            ttl=120,\n",
    "            distance_threshold=0.2,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "    def initialize(self, documents: List[str]):\n",
    "        \"\"\"Initialize RAG with documents\"\"\"\n",
    "\n",
    "        self.vector_store.add_documents(\n",
    "            docs=documents, embedding_engine=self.embedding_engine\n",
    "        )\n",
    "\n",
    "    def query(self, question: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Execute RAG query with retrieval and generation\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        query_vector = self.llmcache._vectorizer.embed(question)\n",
    "        if cache_result := self.llmcache.check(vector=query_vector):\n",
    "            print(\"Cache hit!\")\n",
    "            return cache_result[0][\"response\"]\n",
    "        query_embedding = self.embedding_engine.embed(question)\n",
    "        retrieved_docs = self.vector_store.semantic_search(query_embedding, top_k=top_k)\n",
    "\n",
    "        context = \"\\n\".join([doc for doc in retrieved_docs[\"content\"]])\n",
    "\n",
    "        prompt = f\"\"\"Context:\\n{context}\\nQuestion:\\n{question}\\nAnswer:\\n\"\"\"\n",
    "\n",
    "        cache_key = f\"llm_cache:{question}\"\n",
    "        answer = self.llm.generate(prompt, max_tokens=150, cache_key=cache_key)\n",
    "        self.llmcache.store(question, answer, query_vector)\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "108061dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:40:06 redisvl.index.index INFO   Index already exists, overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created index 'redisvl'\n",
      "22:40:58 root WARNING   Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "22:40:58 sentence_transformers.SentenceTransformer INFO   Use pytorch device_name: cuda:0\n",
      "22:40:58 sentence_transformers.SentenceTransformer INFO   Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "22:41:02 redisvl.index.index INFO   Index already exists, overwriting.\n",
      "‚úì Added 5 documents to vector store\n"
     ]
    }
   ],
   "source": [
    "sem_rag = SemaRAGPipeline()\n",
    "sem_rag.initialize(RAG_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae556f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Query: describe Machine learning\n",
      "Cache hit!\n",
      "Context:\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "Deep learning uses neural networks with multiple layers to process data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\n",
      "Natural language processing (NLP) is a branch of AI that helps computers understand, interpret, and generate human language. It powers chatbots, translation services, and sentiment analysis.\n",
      "Question:\n",
      "describe Machine learning\n",
      "Answer:\n",
      "The answer is: Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on algorithms and statistical models.\n",
      "The answer is: Machine learning is a subset of artificial\n"
     ]
    }
   ],
   "source": [
    "query = \"describe Machine learning\"\n",
    "print(f\"\\n‚ùì Query: {query}\")\n",
    "result = sem_rag.query(query)\n",
    "if isinstance(result,dict):\n",
    "    print(f\"\\nüìÑ Retrieved Context {result['context']}\")\n",
    "    print(f\"\\nüí¨ Answer:\\n{result['answer']}\")\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ff69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
